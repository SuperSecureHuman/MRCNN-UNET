import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms.functional as TF

import math

############################################################
# Hyperparameters
############################################################

# Backbone network architecture
# Supported values are: resnet50, resnet101
BACKBONE = "resnet50"

# Length of square anchor side in pixels
RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)

# Ratios of anchors at each cell (width/height)
# A value of 1 represents a square anchor, and 0.5 is a wide anchor
RPN_ANCHOR_RATIOS = [0.5, 1, 2]
RPN_ANCHOR_LEN = len(RPN_ANCHOR_RATIOS)

# Anchor stride
# If 1 then anchors are created for each cell in the backbone feature map.
# If 2, then anchors are created for every other cell, and so on.
RPN_ANCHOR_STRIDE = 1

# Non-max suppression threshold to filter RPN proposals.
# You can increase this during training to generate more propsals.
RPN_NMS_THRESHOLD = 0.7

# ROIs kept after non-maximum supression (training and inference)
POST_NMS_ROIS = 2000



class FPN(nn.Module):
    """
    Inputs = C1, C2, C3, C4, C5 from any feature extractor backbone (here resnet)
             out_channels = number of channels in each of output feature maps
    Outputs = P2, P3, P4, P5, P6 - Feature maps of different sizes
    """

    def __init__(self, C1, C2, C3, C4, C5, out_channels):
        super(FPN, self).__init__()
        self.out_channels = out_channels
        # C1 - C5 feature extractor backbone layers
        # Imagine like breaking any backbone into 5 blocks
        self.C1 = C1
        self.C2 = C2
        self.C3 = C3
        self.C4 = C4
        self.C5 = C5

        # All the P# layers are explained below

        self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)
        self.P5_conv1 = nn.Conv2d(
            2048, self.out_channels, kernel_size=1, stride=1)
        self.P5_conv2 = nn.Sequential(
            nn.Conv2d(self.out_channels, self.out_channels,
                      kernel_size=3, stride=1, padding='same')
        )
        self.P4_conv1 = nn.Conv2d(
            1024, self.out_channels, kernel_size=1, stride=1)
        self.P4_conv2 = nn.Sequential(
            nn.Conv2d(self.out_channels, self.out_channels,
                      kernel_size=3, stride=1, padding='same')
        )
        self.P3_conv1 = nn.Conv2d(
            512, self.out_channels, kernel_size=1, stride=1)
        self.P3_conv2 = nn.Sequential(
            nn.Conv2d(self.out_channels, self.out_channels,
                      kernel_size=3, stride=1, padding='same')
        )
        self.P2_conv1 = nn.Conv2d(
            256, self.out_channels, kernel_size=1, stride=1)
        self.P2_conv2 = nn.Sequential(
            nn.Conv2d(self.out_channels, self.out_channels,
                      kernel_size=3, stride=1, padding='same')
        )

    def forward(self, x):
        # In FPN, we have residual link C2,C3,C4 and C5 to P2,P3,P4,P5
        x = self.C1(x)
        x = self.C2(x)
        c2_out = x
        x = self.C3(x)
        c3_out = x
        x = self.C4(x)
        c4_out = x
        x = self.C5(x)
        # The following lines - From the TOP down layers, we are doing the following:
        # 1. Convolution with 1x1 kernel to reduce the number of channels (conv1)
        # 2. Upsampling the layer to the size of the next layer (F.interpolate(layer, scale_factor=2))
        # 3. Adding the residual link (layer + F.interpolate(layer, scale_factor=2))
        # 4. Convolution with 3x3 kernel to get the final output (conv2)
        p5_out = self.P5_conv2(self.P5_conv1(x))
        p4_out = self.P4_conv2(self.P4_conv1(c4_out) +
                               F.interpolate(p5_out, scale_factor=2))
        p3_out = self.P3_conv2(self.P3_conv1(c3_out) +
                               F.interpolate(p4_out, scale_factor=2))
        p2_out = self.P2_conv2(self.P2_conv1(c2_out) +
                               F.interpolate(p3_out, scale_factor=2))

        # P6 is used for the 5th anchor scale in RPN. Generated by
        # subsampling from P5 with stride of 2.
        p6_out = self.P6(p5_out)

        return [p2_out, p3_out, p4_out, p5_out, p6_out]


# For Resnet
# TODO: Logic of Expansions

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding='same')
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, architecture, stage5=False):
        super(ResNet, self).__init__()
        assert architecture in ["resnet50", "resnet101"]
        self.inplanes = 64
        self.layers = [3, 4, {"resnet50": 6, "resnet101": 23}[architecture], 3]
        self.block = Bottleneck
        self.stage5 = stage5

        self.C1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True))

        self.C2 = self.make_layer(self.block, 64, self.layers[0])
        self.C3 = self.make_layer(self.block, 128, self.layers[1], stride=2)
        self.C4 = self.make_layer(self.block, 256, self.layers[2], stride=2)
        if self.stage5:
            self.C5 = self.make_layer(
                self.block, 512, self.layers[3], stride=2)
        else:
            self.C5 = None

    def forward(self, x):
        x = self.C1(x)
        x = self.C2(x)
        x = self.C3(x)
        x = self.C4(x)
        x = self.C5(x)
        return x

    def stages(self):
        return [self.C1, self.C2, self.C3, self.C4, self.C5]

    def make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)


class RPN(nn.Module):
    '''
    Region Proposal Network

    RPN init function
    Args:
        anchors_per_location: Number of anchors per pixel in the feature map
        anchor_stride: Stride of the feature map relative to the image in pixels
        depth: Depth of the feature map coming from FPN
    
    Returns:
        None

    RPN forward function
    Args:
        x: Feature map from FPN
    
    Returns:
        rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax)
        rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.
        rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be
    '''

    def __init__(self, anchors_per_location, anchor_stride, depth):
        super(RPN, self).__init__()
        self.anchors_per_location = anchors_per_location
        self.anchor_stride = anchor_stride
        self.depth = depth

        self.conv_shared = nn.Conv2d(
            self.depth, 512, kernel_size=3, stride=self.anchor_stride, padding="same")
        self.relu = nn.ReLU(inplace=True)
        self.conv_class = nn.Conv2d(
            512, 2 * anchors_per_location, kernel_size=1, stride=1)
        self.softmax = nn.Softmax(dim=2)
        self.conv_bbox = nn.Conv2d(
            512, 4 * anchors_per_location, kernel_size=1, stride=1)

    def forward(self, x):
        # Shared convolutional base of the RPN
        x = self.relu(self.conv_shared(x))

        # Anchor Score. [batch, anchors per location * 2, height, width].
        rpn_class_logits = self.conv_class(x)

        # Reshape to [batch, 2, anchors]
        rpn_class_logits = rpn_class_logits.permute(0, 2, 3, 1)
        rpn_class_logits = rpn_class_logits.contiguous()
        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)

        # Softmax on last dimension of BG/FG.
        rpn_probs = self.softmax(rpn_class_logits)

        # Bounding box refinement. [batch, H, W, anchors per location, depth]
        # where depth is [x, y, log(w), log(h)]
        rpn_bbox = self.conv_bbox(x)

        # Reshape to [batch, 4, anchors]
        rpn_bbox = rpn_bbox.permute(0, 2, 3, 1)
        rpn_bbox = rpn_bbox.contiguous()
        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)

        return [rpn_class_logits, rpn_probs, rpn_bbox]


x = torch.randn(1, 3, 224, 224)
resnet = ResNet(BACKBONE, stage5=True)
C1, C2, C3, C4, C5 = resnet.stages()
fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)
[p2,p3,p4,p5,p6] = fpn(x)
rpn = RPN(anchors_per_location=RPN_ANCHOR_LEN, anchor_stride=RPN_ANCHOR_STRIDE, depth=256) # we can use anchor ratios of [0.5,1,2]

rpn_feature_maps = [p2, p3, p4, p5, p6] # We use p6 in RPN 
mrcnn_feature_maps = [p2, p3, p4, p5]

rpn_outputs = []
for rpn_input in rpn_feature_maps:
    rpn_outputs.append(rpn(rpn_input))

# Concatenate layer outputs
# Convert from list of lists of level outputs to list of lists
# of outputs across levels.
# e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]
outputs = list(zip(*rpn_outputs))
outputs = [torch.cat(list(o), dim=1) for o in outputs]
rpn_class_logits, rpn_class, rpn_bbox = outputs

